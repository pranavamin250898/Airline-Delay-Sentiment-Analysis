{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airline Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement: The task is to perform a sentiment analysis on the tweets about the problems of each major U.S. airline. The data was procured from Twitter from February 2015 and the contributers were asked to classify each tweet into one of these categories: Positive, Negative and netural tweets, following by categorizing negative reasons (such as \"late flight\" or \"rude service\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer Service is one of the most important aspect of business management. Customer service is important because it inspires customer loyalty and makes employee's jobs easier. This, in turn, helps businesses grow. By providing great customer service, organizations can recover customer acquisition costs, retain talent, and foster brand loyalty. Therefore, analysing the consumer sentiment is the integral part of Customer Service. Since, in the modern world, people express their excitement, sorrows, regret, anger etc. of a public or private service using Twitter, we believe that tweets can be considered a fairly reliable source of consumer sentiment. By performing this analysis, we will be arming ourselves with key information as to how to improve the negative aspects of our Airline services, as well maintain and upgrade the positive ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gupta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Regular Expression: For pre-processing of data i.e. finding and replacing patterns from strings\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Tokenizer: Used to vectorize a sequence of text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# pad_sequences: This function transforms a list of integers into 2D Numpy array\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Splits entire dataset into training and testing sets based on a given percentage\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Optimizers\n",
    "from keras.optimizers import SGD, Adagrad, RMSprop, Adam\n",
    "# Scikit-learn classifier implementation for Keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# GridSearchCV: Module to compute best configurations for models\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Regularizers: Used to apply penalties on layer parameters or layer activity during optimization\n",
    "from keras import regularizers\n",
    "# EarlyStopping: Used to stop training when a monitored metric has stopped improving in order to save time\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Metrics: Used to evaluate and compare machine learning models\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve,auc\n",
    "\n",
    "# NLTK: Natural Language Toolkit\n",
    "import nltk\n",
    "# Stopwords: Used to identify and remove common words that don't hold considerable sentiment\n",
    "from nltk.corpus import stopwords\n",
    "# word_tokenize: Used to convert a sentence into a sequence of words i.e. tokens\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Used for the Stemming and Lemmatization of the tokens\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Deep learning libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing is an important step in Natural Language Processing (NLP) because it helps to improve the quality of the input data and prepare it for further processing. Pre-processing is important for NLP tasks as it helps in:\n",
    "\n",
    "Data Cleaning: Raw text data often contains noise and inconsistencies that can negatively affect the performance of NLP models. Pre-processing helps to clean the text data and remove irrelevant information.\n",
    "\n",
    "Dimensionality Reduction: Pre-processing can help to reduce the dimensionality of the data by transforming words into their root form, removing stop words, and reducing the case sensitivity of the data. This reduces the size of the data and helps the model to learn more effectively.\n",
    "\n",
    "Improved Performance: By removing noise and inconsistencies in the data, pre-processing can improve the performance of NLP models. This is particularly important for tasks such as sentiment analysis, where even small inconsistencies in the data can have a big impact on the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Remove @username\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-z0-9\\s]', '', text)\n",
    "    # Tokenize the words\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words= [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
    "        \n",
    "    # Join the words\n",
    "    text = ' '.join(lemmatized_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the tweets data\n",
    "data = pd.read_csv(\"Tweets.csv\")\n",
    "# Removing unsignificant columns\n",
    "df = data[[\"text\",\"airline_sentiment\"]]\n",
    "# Keeping \"Positive\" and \"Negative\" tweets only\n",
    "df = df[df.airline_sentiment != \"neutral\"]\n",
    "\n",
    "# Pre-processing the imported data\n",
    "df['text'] = df['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating 4000 dimensions vectors from each 'text' objects\n",
    "max_features = 4000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(df['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating independent and dependent variables into separate variables\n",
    "X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "X = pad_sequences(X)\n",
    "Y = np.array(df['airline_sentiment'] == \"positive\", dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and testing data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(optimizer):\n",
    "    # Creating a sequential model\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Embedding layer: To map the high-dimensional one-hot encoded vectors of input words in a vocabulary to low-dimensional dense vectors (word embeddings)\n",
    "    model.add(tf.keras.layers.Embedding(max_features, 128, input_length=X_train.shape[1]))\n",
    "\n",
    "    # SpatialDropout1D: Responsible for dropping out entire sequences of word vectors during each forward pass of the model. This helps in generalizing the input, so that model is less prone to be overfit.\n",
    "    model.add(tf.keras.layers.SpatialDropout1D(0.5))\n",
    "    \n",
    "    # Added dropout parameter to drop 30% of the units for the linear transformation of the inputs\n",
    "    # Added recurrent_dropout parameter to drop 30% of the units for the linear transformation of the reecurrent state\n",
    "    model.add(tf.keras.layers.LSTM(196, dropout = 0.3, recurrent_dropout = 0.3 ))\n",
    "\n",
    "    # Dropout: Eliminates 20% of the input units to help avoid overfitting\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    # Dense layer: Layer with 100 neurons for attribute recognition in the text sequences\n",
    "    model.add(tf.keras.layers.Dense(100, activation = tf.nn.relu))\n",
    "    \n",
    "    # Dropout: Eliminates 20% of the input units to help avoid overfitting\n",
    "    model.add(tf.keras.layers.Dropout(0.4))\n",
    "\n",
    "    # Dense: Final layer with 2 neurons i.e. \"Positive\" and \"Negative\" with softmax activation function\n",
    "    # Added the L1 regularization i.e. Lasso regression to the dense layer to additionally penalize the procured loss\n",
    "    model.add(tf.keras.layers.Dense(2, activation = tf.nn.softmax,kernel_regularizer=regularizers.L1(0.01)))\n",
    "\n",
    "    # Compiling model with given optimizer, loss and metrics\n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gupta\\AppData\\Local\\Temp\\ipykernel_3256\\909094409.py:1: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=build_classifier)\n"
     ]
    }
   ],
   "source": [
    "model = KerasClassifier(build_fn=build_classifier)\n",
    "\n",
    "# Defining parameter choices in the form of a dictionary\n",
    "parameters = {\n",
    "                'optimizer': ['RMSprop', 'Adam', 'Adamax', 'Nadam', 'Nesterov','Adagrad'],\n",
    "                'batch_size': [32, 64],\n",
    "                'epochs': [8, 10]\n",
    "             }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearchCV function takes as input a machine learning model, a set of hyperparameter values to search over, and a scoring metric. It then trains the model using each combination of hyperparameters and evaluates the model using the specified scoring metric. The best combination of hyperparameters is then selected based on the highest score achieved by the model. Hence, we use GridSearchCV for hyperparamter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Defining the GridSearchCV object with the model and the given parameters\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "404/404 [==============================] - 19s 43ms/step - loss: 0.4539 - accuracy: 0.8497\n",
      "Epoch 2/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2398 - accuracy: 0.9242\n",
      "Epoch 3/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1864 - accuracy: 0.9434\n",
      "Epoch 4/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1490 - accuracy: 0.9530\n",
      "Epoch 5/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1267 - accuracy: 0.9630\n",
      "Epoch 6/8\n",
      "404/404 [==============================] - 17s 42ms/step - loss: 0.1055 - accuracy: 0.9681\n",
      "Epoch 7/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0988 - accuracy: 0.9698\n",
      "Epoch 8/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0830 - accuracy: 0.9785\n",
      "51/51 [==============================] - 1s 9ms/step\n",
      "Epoch 1/8\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.4545 - accuracy: 0.8538\n",
      "Epoch 2/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2400 - accuracy: 0.9197\n",
      "Epoch 3/8\n",
      "404/404 [==============================] - 17s 42ms/step - loss: 0.1851 - accuracy: 0.9415\n",
      "Epoch 4/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1516 - accuracy: 0.9522\n",
      "Epoch 5/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1217 - accuracy: 0.9632\n",
      "Epoch 6/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1065 - accuracy: 0.9701\n",
      "Epoch 7/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0937 - accuracy: 0.9742\n",
      "Epoch 8/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0872 - accuracy: 0.9749\n",
      "51/51 [==============================] - 1s 9ms/step\n",
      "Epoch 1/8\n",
      "404/404 [==============================] - 19s 43ms/step - loss: 0.4620 - accuracy: 0.8504\n",
      "Epoch 2/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2409 - accuracy: 0.9220\n",
      "Epoch 3/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1797 - accuracy: 0.9438\n",
      "Epoch 4/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1446 - accuracy: 0.9559\n",
      "Epoch 5/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1219 - accuracy: 0.9660\n",
      "Epoch 6/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1041 - accuracy: 0.9698\n",
      "Epoch 7/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0945 - accuracy: 0.9735\n",
      "Epoch 8/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0822 - accuracy: 0.9766\n",
      "51/51 [==============================] - 1s 9ms/step\n",
      "Epoch 1/8\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.4539 - accuracy: 0.8524\n",
      "Epoch 2/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2411 - accuracy: 0.9214\n",
      "Epoch 3/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1833 - accuracy: 0.9438\n",
      "Epoch 4/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1481 - accuracy: 0.9559\n",
      "Epoch 5/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1186 - accuracy: 0.9641\n",
      "Epoch 6/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0992 - accuracy: 0.9709\n",
      "Epoch 7/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0848 - accuracy: 0.9756\n",
      "Epoch 8/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0780 - accuracy: 0.9794\n",
      "51/51 [==============================] - 1s 9ms/step\n",
      "Epoch 1/8\n",
      "404/404 [==============================] - 19s 43ms/step - loss: 0.4528 - accuracy: 0.8570\n",
      "Epoch 2/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2372 - accuracy: 0.9263\n",
      "Epoch 3/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1753 - accuracy: 0.9469\n",
      "Epoch 4/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1470 - accuracy: 0.9570\n",
      "Epoch 5/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1205 - accuracy: 0.9643\n",
      "Epoch 6/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1105 - accuracy: 0.9709\n",
      "Epoch 7/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0907 - accuracy: 0.9760\n",
      "Epoch 8/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0814 - accuracy: 0.9777\n",
      "51/51 [==============================] - 1s 9ms/step\n",
      "Epoch 1/8\n",
      "404/404 [==============================] - 18s 42ms/step - loss: 0.4787 - accuracy: 0.8451\n",
      "Epoch 2/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.3123 - accuracy: 0.8904\n",
      "Epoch 3/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2645 - accuracy: 0.9059\n",
      "Epoch 4/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2423 - accuracy: 0.9166\n",
      "Epoch 5/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2291 - accuracy: 0.9200\n",
      "Epoch 6/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2154 - accuracy: 0.9248\n",
      "Epoch 7/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2092 - accuracy: 0.9265\n",
      "Epoch 8/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2019 - accuracy: 0.9284\n",
      "51/51 [==============================] - 1s 10ms/step\n",
      "Epoch 1/8\n",
      "404/404 [==============================] - 19s 43ms/step - loss: 0.4757 - accuracy: 0.8428\n",
      "Epoch 2/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.3088 - accuracy: 0.8969\n",
      "Epoch 3/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2649 - accuracy: 0.9059\n",
      "Epoch 4/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2452 - accuracy: 0.9141\n",
      "Epoch 5/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2312 - accuracy: 0.9169\n",
      "Epoch 6/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2184 - accuracy: 0.9249\n",
      "Epoch 7/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2122 - accuracy: 0.9254\n",
      "Epoch 8/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2042 - accuracy: 0.9321\n",
      "51/51 [==============================] - 1s 10ms/step\n",
      "Epoch 1/8\n",
      "404/404 [==============================] - 19s 43ms/step - loss: 0.4738 - accuracy: 0.8451\n",
      "Epoch 2/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.3084 - accuracy: 0.8945\n",
      "Epoch 3/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2703 - accuracy: 0.9027\n",
      "Epoch 4/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2507 - accuracy: 0.9089\n",
      "Epoch 5/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2286 - accuracy: 0.9169\n",
      "Epoch 6/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2162 - accuracy: 0.9200\n",
      "Epoch 7/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2092 - accuracy: 0.9253\n",
      "Epoch 8/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2039 - accuracy: 0.9291\n",
      "51/51 [==============================] - 1s 11ms/step\n",
      "Epoch 1/8\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.4664 - accuracy: 0.8450\n",
      "Epoch 2/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.3066 - accuracy: 0.8948\n",
      "Epoch 3/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2623 - accuracy: 0.9073\n",
      "Epoch 4/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2414 - accuracy: 0.9164\n",
      "Epoch 5/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2287 - accuracy: 0.9208\n",
      "Epoch 6/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2211 - accuracy: 0.9219\n",
      "Epoch 7/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2106 - accuracy: 0.9268\n",
      "Epoch 8/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2041 - accuracy: 0.9305\n",
      "51/51 [==============================] - 1s 10ms/step\n",
      "Epoch 1/8\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.4703 - accuracy: 0.8467\n",
      "Epoch 2/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.3033 - accuracy: 0.8929\n",
      "Epoch 3/8\n",
      "404/404 [==============================] - 17s 42ms/step - loss: 0.2570 - accuracy: 0.9084\n",
      "Epoch 4/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2377 - accuracy: 0.9140\n",
      "Epoch 5/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2182 - accuracy: 0.9246\n",
      "Epoch 6/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2148 - accuracy: 0.9228\n",
      "Epoch 7/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1994 - accuracy: 0.9305\n",
      "Epoch 8/8\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1952 - accuracy: 0.9335\n",
      "51/51 [==============================] - 1s 9ms/step\n",
      "Epoch 1/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.4591 - accuracy: 0.8517\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2412 - accuracy: 0.9236\n",
      "Epoch 3/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1858 - accuracy: 0.9432\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1501 - accuracy: 0.9522\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1279 - accuracy: 0.9602\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1022 - accuracy: 0.9681\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0949 - accuracy: 0.9708\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0806 - accuracy: 0.9780\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0780 - accuracy: 0.9791\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0682 - accuracy: 0.9805\n",
      "51/51 [==============================] - 1s 10ms/step\n",
      "Epoch 1/10\n",
      "404/404 [==============================] - 19s 43ms/step - loss: 0.4457 - accuracy: 0.8589\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2424 - accuracy: 0.9217\n",
      "Epoch 3/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1733 - accuracy: 0.9446\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1458 - accuracy: 0.9548\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.1233 - accuracy: 0.9632\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1048 - accuracy: 0.9678\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0887 - accuracy: 0.9774\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0829 - accuracy: 0.9755\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.0762 - accuracy: 0.9780\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.0616 - accuracy: 0.9830\n",
      "51/51 [==============================] - 1s 12ms/step\n",
      "Epoch 1/10\n",
      "404/404 [==============================] - 19s 43ms/step - loss: 0.4459 - accuracy: 0.8578\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.2342 - accuracy: 0.9254\n",
      "Epoch 3/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.1744 - accuracy: 0.9455\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.1463 - accuracy: 0.9545\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.1189 - accuracy: 0.9660\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - 18s 44ms/step - loss: 0.0974 - accuracy: 0.9720\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.0837 - accuracy: 0.9769\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.0800 - accuracy: 0.9776\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.0721 - accuracy: 0.9800\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.0641 - accuracy: 0.9819\n",
      "51/51 [==============================] - 1s 12ms/step\n",
      "Epoch 1/10\n",
      "404/404 [==============================] - 19s 44ms/step - loss: 0.4626 - accuracy: 0.8538\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - 18s 44ms/step - loss: 0.2435 - accuracy: 0.9181\n",
      "Epoch 3/10\n",
      "404/404 [==============================] - 18s 44ms/step - loss: 0.1808 - accuracy: 0.9438\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - 18s 44ms/step - loss: 0.1431 - accuracy: 0.9557\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - 18s 44ms/step - loss: 0.1176 - accuracy: 0.9647\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - 18s 44ms/step - loss: 0.0948 - accuracy: 0.9729\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - 18s 44ms/step - loss: 0.0889 - accuracy: 0.9743\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - 18s 44ms/step - loss: 0.0778 - accuracy: 0.9782\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - 18s 44ms/step - loss: 0.0712 - accuracy: 0.9828\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - 18s 44ms/step - loss: 0.0612 - accuracy: 0.9834\n",
      "51/51 [==============================] - 1s 12ms/step\n",
      "Epoch 1/10\n",
      "404/404 [==============================] - 19s 43ms/step - loss: 0.4428 - accuracy: 0.8561\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2398 - accuracy: 0.9237\n",
      "Epoch 3/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1818 - accuracy: 0.9435\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1414 - accuracy: 0.9584\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1211 - accuracy: 0.9636\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - 18s 44ms/step - loss: 0.0998 - accuracy: 0.9715\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - 18s 44ms/step - loss: 0.0856 - accuracy: 0.9746\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.0822 - accuracy: 0.9762\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.0693 - accuracy: 0.9802\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.0653 - accuracy: 0.9824\n",
      "51/51 [==============================] - 1s 10ms/step\n",
      "Epoch 1/10\n",
      "404/404 [==============================] - 19s 43ms/step - loss: 0.4771 - accuracy: 0.8451\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.3082 - accuracy: 0.8924\n",
      "Epoch 3/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2653 - accuracy: 0.9031\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2493 - accuracy: 0.9115\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2305 - accuracy: 0.9201\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2193 - accuracy: 0.9219\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2048 - accuracy: 0.9271\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2039 - accuracy: 0.9305\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1944 - accuracy: 0.9324\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1902 - accuracy: 0.9333\n",
      "51/51 [==============================] - 1s 10ms/step\n",
      "Epoch 1/10\n",
      "404/404 [==============================] - 19s 43ms/step - loss: 0.4726 - accuracy: 0.8474\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.3044 - accuracy: 0.8920\n",
      "Epoch 3/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2592 - accuracy: 0.9095\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2452 - accuracy: 0.9130\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2270 - accuracy: 0.9180\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2204 - accuracy: 0.9253\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2040 - accuracy: 0.9268\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1999 - accuracy: 0.9284\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1948 - accuracy: 0.9342\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1882 - accuracy: 0.9335\n",
      "51/51 [==============================] - 1s 10ms/step\n",
      "Epoch 1/10\n",
      "404/404 [==============================] - 18s 42ms/step - loss: 0.4807 - accuracy: 0.8403\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.3085 - accuracy: 0.8934\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2669 - accuracy: 0.9070\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2465 - accuracy: 0.9126\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2321 - accuracy: 0.9192\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2221 - accuracy: 0.9259\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2106 - accuracy: 0.9246\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2056 - accuracy: 0.9290\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1977 - accuracy: 0.9282\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1884 - accuracy: 0.9331\n",
      "51/51 [==============================] - 1s 9ms/step\n",
      "Epoch 1/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.4734 - accuracy: 0.8428\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.3071 - accuracy: 0.8954\n",
      "Epoch 3/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2656 - accuracy: 0.9079\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2421 - accuracy: 0.9137\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2283 - accuracy: 0.9174\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2245 - accuracy: 0.9195\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2127 - accuracy: 0.9276\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2070 - accuracy: 0.9315\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1948 - accuracy: 0.9350\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.1888 - accuracy: 0.9373\n",
      "51/51 [==============================] - 1s 9ms/step\n",
      "Epoch 1/10\n",
      "404/404 [==============================] - 18s 43ms/step - loss: 0.4681 - accuracy: 0.8467\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - 17s 42ms/step - loss: 0.3018 - accuracy: 0.8943\n",
      "Epoch 3/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2564 - accuracy: 0.9113\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 0.2360 - accuracy: 0.9168\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - 19s 47ms/step - loss: 0.2238 - accuracy: 0.9205\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - 23s 57ms/step - loss: 0.2099 - accuracy: 0.9259\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - 32s 79ms/step - loss: 0.2032 - accuracy: 0.9311\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - 31s 77ms/step - loss: 0.1994 - accuracy: 0.9308\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - 31s 77ms/step - loss: 0.1916 - accuracy: 0.9332\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - 31s 77ms/step - loss: 0.1840 - accuracy: 0.9395\n",
      "51/51 [==============================] - 1s 20ms/step\n",
      "Epoch 1/8\n",
      "202/202 [==============================] - 21s 91ms/step - loss: 0.5095 - accuracy: 0.8383\n",
      "Epoch 2/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2689 - accuracy: 0.9126\n",
      "Epoch 3/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1987 - accuracy: 0.9417\n",
      "Epoch 4/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1581 - accuracy: 0.9525\n",
      "Epoch 5/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1361 - accuracy: 0.9607\n",
      "Epoch 6/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1112 - accuracy: 0.9666\n",
      "Epoch 7/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0990 - accuracy: 0.9721\n",
      "Epoch 8/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.0905 - accuracy: 0.9743\n",
      "51/51 [==============================] - 1s 21ms/step\n",
      "Epoch 1/8\n",
      "202/202 [==============================] - 20s 89ms/step - loss: 0.5100 - accuracy: 0.8408\n",
      "Epoch 2/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.2577 - accuracy: 0.9201\n",
      "Epoch 3/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.2023 - accuracy: 0.9367\n",
      "Epoch 4/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.1670 - accuracy: 0.9486\n",
      "Epoch 5/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.1327 - accuracy: 0.9624\n",
      "Epoch 6/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.1152 - accuracy: 0.9667\n",
      "Epoch 7/8\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1015 - accuracy: 0.9706\n",
      "Epoch 8/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.0848 - accuracy: 0.9774\n",
      "51/51 [==============================] - 1s 20ms/step\n",
      "Epoch 1/8\n",
      "202/202 [==============================] - 20s 90ms/step - loss: 0.5256 - accuracy: 0.8420\n",
      "Epoch 2/8\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.2629 - accuracy: 0.9208\n",
      "Epoch 3/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.1970 - accuracy: 0.9393\n",
      "Epoch 4/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.1639 - accuracy: 0.9528\n",
      "Epoch 5/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.1338 - accuracy: 0.9599\n",
      "Epoch 6/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.1155 - accuracy: 0.9681\n",
      "Epoch 7/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.0978 - accuracy: 0.9715\n",
      "Epoch 8/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.0864 - accuracy: 0.9737\n",
      "51/51 [==============================] - 1s 20ms/step\n",
      "Epoch 1/8\n",
      "202/202 [==============================] - 20s 89ms/step - loss: 0.5126 - accuracy: 0.8460\n",
      "Epoch 2/8\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.2645 - accuracy: 0.9208\n",
      "Epoch 3/8\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1943 - accuracy: 0.9428\n",
      "Epoch 4/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.1564 - accuracy: 0.9499\n",
      "Epoch 5/8\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1320 - accuracy: 0.9609\n",
      "Epoch 6/8\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1183 - accuracy: 0.9661\n",
      "Epoch 7/8\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.0969 - accuracy: 0.9729\n",
      "Epoch 8/8\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.0831 - accuracy: 0.9803\n",
      "51/51 [==============================] - 1s 20ms/step\n",
      "Epoch 1/8\n",
      "202/202 [==============================] - 20s 89ms/step - loss: 0.5055 - accuracy: 0.8422\n",
      "Epoch 2/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.2582 - accuracy: 0.9211\n",
      "Epoch 3/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.1976 - accuracy: 0.9404\n",
      "Epoch 4/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.1598 - accuracy: 0.9527\n",
      "Epoch 5/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.1290 - accuracy: 0.9622\n",
      "Epoch 6/8\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1081 - accuracy: 0.9692\n",
      "Epoch 7/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.0978 - accuracy: 0.9720\n",
      "Epoch 8/8\n",
      "202/202 [==============================] - 18s 89ms/step - loss: 0.0840 - accuracy: 0.9766\n",
      "51/51 [==============================] - 1s 20ms/step\n",
      "Epoch 1/8\n",
      "202/202 [==============================] - 21s 91ms/step - loss: 0.5196 - accuracy: 0.8392\n",
      "Epoch 2/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.3186 - accuracy: 0.8962\n",
      "Epoch 3/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.2562 - accuracy: 0.9141\n",
      "Epoch 4/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.2312 - accuracy: 0.9188\n",
      "Epoch 5/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.2161 - accuracy: 0.9268\n",
      "Epoch 6/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1997 - accuracy: 0.9302\n",
      "Epoch 7/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1939 - accuracy: 0.9310\n",
      "Epoch 8/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1859 - accuracy: 0.9376\n",
      "51/51 [==============================] - 1s 21ms/step\n",
      "Epoch 1/8\n",
      "202/202 [==============================] - 20s 91ms/step - loss: 0.5193 - accuracy: 0.8389\n",
      "Epoch 2/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.3089 - accuracy: 0.9016\n",
      "Epoch 3/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.2618 - accuracy: 0.9141\n",
      "Epoch 4/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.2339 - accuracy: 0.9226\n",
      "Epoch 5/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.2120 - accuracy: 0.9277\n",
      "Epoch 6/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.2067 - accuracy: 0.9291\n",
      "Epoch 7/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1965 - accuracy: 0.9341\n",
      "Epoch 8/8\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1844 - accuracy: 0.9393\n",
      "51/51 [==============================] - 1s 21ms/step\n",
      "Epoch 1/8\n",
      "202/202 [==============================] - 21s 92ms/step - loss: 0.5231 - accuracy: 0.8366\n",
      "Epoch 2/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.3079 - accuracy: 0.9006\n",
      "Epoch 3/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2552 - accuracy: 0.9141\n",
      "Epoch 4/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2318 - accuracy: 0.9206\n",
      "Epoch 5/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2186 - accuracy: 0.9232\n",
      "Epoch 6/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2037 - accuracy: 0.9305\n",
      "Epoch 7/8\n",
      "202/202 [==============================] - 19s 93ms/step - loss: 0.1971 - accuracy: 0.9304\n",
      "Epoch 8/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1869 - accuracy: 0.9336\n",
      "51/51 [==============================] - 1s 22ms/step\n",
      "Epoch 1/8\n",
      "202/202 [==============================] - 21s 92ms/step - loss: 0.5160 - accuracy: 0.8397\n",
      "Epoch 2/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.3100 - accuracy: 0.9007\n",
      "Epoch 3/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2637 - accuracy: 0.9151\n",
      "Epoch 4/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2276 - accuracy: 0.9223\n",
      "Epoch 5/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2144 - accuracy: 0.9273\n",
      "Epoch 6/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2002 - accuracy: 0.9319\n",
      "Epoch 7/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1896 - accuracy: 0.9342\n",
      "Epoch 8/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1783 - accuracy: 0.9390\n",
      "51/51 [==============================] - 1s 22ms/step\n",
      "Epoch 1/8\n",
      "202/202 [==============================] - 21s 92ms/step - loss: 0.5210 - accuracy: 0.8378\n",
      "Epoch 2/8\n",
      "202/202 [==============================] - 19s 95ms/step - loss: 0.3079 - accuracy: 0.9008\n",
      "Epoch 3/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2563 - accuracy: 0.9158\n",
      "Epoch 4/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2269 - accuracy: 0.9242\n",
      "Epoch 5/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2098 - accuracy: 0.9287\n",
      "Epoch 6/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1977 - accuracy: 0.9296\n",
      "Epoch 7/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1866 - accuracy: 0.9346\n",
      "Epoch 8/8\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1793 - accuracy: 0.9384\n",
      "51/51 [==============================] - 1s 22ms/step\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 20s 92ms/step - loss: 0.5194 - accuracy: 0.8366\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2629 - accuracy: 0.9184\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1997 - accuracy: 0.9410\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1605 - accuracy: 0.9513\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1352 - accuracy: 0.9605\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1121 - accuracy: 0.9650\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0994 - accuracy: 0.9714\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0871 - accuracy: 0.9766\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0870 - accuracy: 0.9768\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0750 - accuracy: 0.9807\n",
      "51/51 [==============================] - 1s 21ms/step\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 21s 92ms/step - loss: 0.5174 - accuracy: 0.8422\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2583 - accuracy: 0.9198\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1964 - accuracy: 0.9400\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1595 - accuracy: 0.9506\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1387 - accuracy: 0.9607\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1120 - accuracy: 0.9683\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0943 - accuracy: 0.9732\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0895 - accuracy: 0.9759\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0805 - accuracy: 0.9785\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0723 - accuracy: 0.9807\n",
      "51/51 [==============================] - 1s 22ms/step\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 21s 92ms/step - loss: 0.5228 - accuracy: 0.8411\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 19s 95ms/step - loss: 0.2630 - accuracy: 0.9177\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 18s 92ms/step - loss: 0.1986 - accuracy: 0.9401\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1499 - accuracy: 0.9534\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1308 - accuracy: 0.9612\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1131 - accuracy: 0.9672\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0925 - accuracy: 0.9717\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0816 - accuracy: 0.9760\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 18s 92ms/step - loss: 0.0736 - accuracy: 0.9814\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0691 - accuracy: 0.9788\n",
      "51/51 [==============================] - 1s 22ms/step\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 20s 91ms/step - loss: 0.5046 - accuracy: 0.8481\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.2697 - accuracy: 0.9130\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1947 - accuracy: 0.9395\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1592 - accuracy: 0.9534\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1282 - accuracy: 0.9615\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1106 - accuracy: 0.9678\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0912 - accuracy: 0.9740\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.0803 - accuracy: 0.9785\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.0789 - accuracy: 0.9774\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.0673 - accuracy: 0.9841\n",
      "51/51 [==============================] - 1s 21ms/step\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 21s 92ms/step - loss: 0.5028 - accuracy: 0.8411\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.2575 - accuracy: 0.9192\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1901 - accuracy: 0.9406\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1543 - accuracy: 0.9522\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1273 - accuracy: 0.9635\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1133 - accuracy: 0.9678\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.0963 - accuracy: 0.9745\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0805 - accuracy: 0.9800\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.0785 - accuracy: 0.9785\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.0691 - accuracy: 0.9821\n",
      "51/51 [==============================] - 1s 22ms/step\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 20s 91ms/step - loss: 0.5202 - accuracy: 0.8400\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.3158 - accuracy: 0.8971\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.2583 - accuracy: 0.9175\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.2334 - accuracy: 0.9203\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.2129 - accuracy: 0.9299\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.2022 - accuracy: 0.9290\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1905 - accuracy: 0.9352\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1827 - accuracy: 0.9390\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1778 - accuracy: 0.9384\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1692 - accuracy: 0.9427\n",
      "51/51 [==============================] - 1s 21ms/step\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 20s 91ms/step - loss: 0.5385 - accuracy: 0.8366\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.3170 - accuracy: 0.8983\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.2650 - accuracy: 0.9135\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.2343 - accuracy: 0.9209\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.2154 - accuracy: 0.9276\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.2018 - accuracy: 0.9301\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 19s 92ms/step - loss: 0.1951 - accuracy: 0.9313\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1874 - accuracy: 0.9370\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1761 - accuracy: 0.9407\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1731 - accuracy: 0.9432\n",
      "51/51 [==============================] - 1s 21ms/step\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 20s 90ms/step - loss: 0.5356 - accuracy: 0.8353\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.3153 - accuracy: 0.8989\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.2657 - accuracy: 0.9119\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.2310 - accuracy: 0.9192\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.2121 - accuracy: 0.9284\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.2024 - accuracy: 0.9296\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1966 - accuracy: 0.9353\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1871 - accuracy: 0.9367\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1770 - accuracy: 0.9389\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1655 - accuracy: 0.9438\n",
      "51/51 [==============================] - 1s 21ms/step\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 19s 90ms/step - loss: 0.5059 - accuracy: 0.8338\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.3058 - accuracy: 0.8971\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.2572 - accuracy: 0.9147\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.2297 - accuracy: 0.9209\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.2148 - accuracy: 0.9250\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.2013 - accuracy: 0.9319\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1942 - accuracy: 0.9315\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1897 - accuracy: 0.9346\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1786 - accuracy: 0.9386\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1752 - accuracy: 0.9423\n",
      "51/51 [==============================] - 1s 20ms/step\n",
      "Epoch 1/10\n",
      "202/202 [==============================] - 20s 90ms/step - loss: 0.5164 - accuracy: 0.8358\n",
      "Epoch 2/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.3088 - accuracy: 0.8996\n",
      "Epoch 3/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.2550 - accuracy: 0.9143\n",
      "Epoch 4/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.2298 - accuracy: 0.9192\n",
      "Epoch 5/10\n",
      "202/202 [==============================] - 19s 93ms/step - loss: 0.2127 - accuracy: 0.9310\n",
      "Epoch 6/10\n",
      "202/202 [==============================] - 18s 92ms/step - loss: 0.1962 - accuracy: 0.9308\n",
      "Epoch 7/10\n",
      "202/202 [==============================] - 18s 91ms/step - loss: 0.1867 - accuracy: 0.9304\n",
      "Epoch 8/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1802 - accuracy: 0.9372\n",
      "Epoch 9/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1682 - accuracy: 0.9406\n",
      "Epoch 10/10\n",
      "202/202 [==============================] - 18s 90ms/step - loss: 0.1645 - accuracy: 0.9434\n",
      "51/51 [==============================] - 1s 21ms/step\n",
      "Epoch 1/10\n",
      "253/253 [==============================] - 25s 90ms/step - loss: 0.4925 - accuracy: 0.8503\n",
      "Epoch 2/10\n",
      "253/253 [==============================] - 23s 90ms/step - loss: 0.2944 - accuracy: 0.9041\n",
      "Epoch 3/10\n",
      "253/253 [==============================] - 23s 90ms/step - loss: 0.2499 - accuracy: 0.9137\n",
      "Epoch 4/10\n",
      "253/253 [==============================] - 23s 90ms/step - loss: 0.2254 - accuracy: 0.9211\n",
      "Epoch 5/10\n",
      "253/253 [==============================] - 23s 90ms/step - loss: 0.2136 - accuracy: 0.9237\n",
      "Epoch 6/10\n",
      "253/253 [==============================] - 23s 90ms/step - loss: 0.2054 - accuracy: 0.9288\n",
      "Epoch 7/10\n",
      "253/253 [==============================] - 23s 90ms/step - loss: 0.1933 - accuracy: 0.9322\n",
      "Epoch 8/10\n",
      "253/253 [==============================] - 23s 90ms/step - loss: 0.1872 - accuracy: 0.9325\n",
      "Epoch 9/10\n",
      "253/253 [==============================] - 23s 90ms/step - loss: 0.1791 - accuracy: 0.9364\n",
      "Epoch 10/10\n",
      "253/253 [==============================] - 23s 90ms/step - loss: 0.1741 - accuracy: 0.9401\n"
     ]
    }
   ],
   "source": [
    "#  Running the Grid search on the given model\n",
    "grid_search = grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Retrieving the best parameters for the given model\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'batch_size': 32, 'epochs': 10, 'optimizer': 'rmsprop'}\n",
      "Best accuracy:  0.9083916255402631\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: \", best_parameters)\n",
    "print(\"Best accuracy: \", best_accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GridSearchCV, we obtain the best configuration for our model based on the given choices of paramter values. We recieved the highest accuracy with the **RMSProp** optimizer, batch size of **32** and **10** epochs, reaching an accuracy of **90.83%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Technique to Handle Imbalance dataset problem:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bagging (Bootstrapped Aggregation) ensemble technique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the bagging ensemble technique in order to procure a better accuracy from an aggregation of the models. Bagging is an ensemble method that trains multiple base models on different bootstrapped samples of the same data. Since each model sees a different random subset of the data, the overall ensemble is less likely to be biased towards the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "177/177 - 20s - loss: 0.5464 - accuracy: 0.8352 - val_loss: 0.4348 - val_accuracy: 0.8700 - 20s/epoch - 111ms/step\n",
      "Epoch 2/10\n",
      "177/177 - 17s - loss: 0.3190 - accuracy: 0.8979 - val_loss: 0.3437 - val_accuracy: 0.8923 - 17s/epoch - 98ms/step\n",
      "Epoch 3/10\n",
      "177/177 - 17s - loss: 0.2604 - accuracy: 0.9163 - val_loss: 0.3307 - val_accuracy: 0.8923 - 17s/epoch - 98ms/step\n",
      "Epoch 4/10\n",
      "177/177 - 17s - loss: 0.2238 - accuracy: 0.9259 - val_loss: 0.2985 - val_accuracy: 0.8944 - 17s/epoch - 98ms/step\n",
      "Epoch 5/10\n",
      "177/177 - 17s - loss: 0.2089 - accuracy: 0.9310 - val_loss: 0.3067 - val_accuracy: 0.8973 - 17s/epoch - 98ms/step\n",
      "Epoch 6/10\n",
      "177/177 - 17s - loss: 0.1924 - accuracy: 0.9358 - val_loss: 0.2772 - val_accuracy: 0.8965 - 17s/epoch - 98ms/step\n",
      "Epoch 7/10\n",
      "177/177 - 17s - loss: 0.1866 - accuracy: 0.9369 - val_loss: 0.2843 - val_accuracy: 0.8940 - 17s/epoch - 98ms/step\n",
      "Epoch 8/10\n",
      "177/177 - 17s - loss: 0.1729 - accuracy: 0.9432 - val_loss: 0.2906 - val_accuracy: 0.8948 - 17s/epoch - 98ms/step\n",
      "Epoch 9/10\n",
      "177/177 - 17s - loss: 0.1661 - accuracy: 0.9445 - val_loss: 0.3125 - val_accuracy: 0.9022 - 17s/epoch - 98ms/step\n",
      "Epoch 10/10\n",
      "177/177 - 17s - loss: 0.1592 - accuracy: 0.9455 - val_loss: 0.3198 - val_accuracy: 0.8998 - 17s/epoch - 98ms/step\n",
      "Epoch 1/10\n",
      "177/177 - 20s - loss: 0.5283 - accuracy: 0.8346 - val_loss: 0.4188 - val_accuracy: 0.8771 - 20s/epoch - 111ms/step\n",
      "Epoch 2/10\n",
      "177/177 - 17s - loss: 0.3109 - accuracy: 0.9018 - val_loss: 0.3106 - val_accuracy: 0.8936 - 17s/epoch - 99ms/step\n",
      "Epoch 3/10\n",
      "177/177 - 18s - loss: 0.2530 - accuracy: 0.9160 - val_loss: 0.2984 - val_accuracy: 0.8932 - 18s/epoch - 99ms/step\n",
      "Epoch 4/10\n",
      "177/177 - 18s - loss: 0.2276 - accuracy: 0.9215 - val_loss: 0.3133 - val_accuracy: 0.8965 - 18s/epoch - 99ms/step\n",
      "Epoch 5/10\n",
      "177/177 - 18s - loss: 0.2038 - accuracy: 0.9307 - val_loss: 0.3028 - val_accuracy: 0.8936 - 18s/epoch - 99ms/step\n",
      "Epoch 6/10\n",
      "177/177 - 18s - loss: 0.1936 - accuracy: 0.9342 - val_loss: 0.2830 - val_accuracy: 0.8981 - 18s/epoch - 99ms/step\n",
      "Epoch 7/10\n",
      "177/177 - 18s - loss: 0.1816 - accuracy: 0.9406 - val_loss: 0.2822 - val_accuracy: 0.8981 - 18s/epoch - 99ms/step\n",
      "Epoch 8/10\n",
      "177/177 - 18s - loss: 0.1768 - accuracy: 0.9404 - val_loss: 0.2799 - val_accuracy: 0.8952 - 18s/epoch - 99ms/step\n",
      "Epoch 9/10\n",
      "177/177 - 18s - loss: 0.1664 - accuracy: 0.9423 - val_loss: 0.2790 - val_accuracy: 0.8907 - 18s/epoch - 99ms/step\n",
      "Epoch 10/10\n",
      "177/177 - 18s - loss: 0.1619 - accuracy: 0.9450 - val_loss: 0.2820 - val_accuracy: 0.9006 - 18s/epoch - 100ms/step\n",
      "Epoch 1/10\n",
      "177/177 - 20s - loss: 0.5559 - accuracy: 0.8350 - val_loss: 0.3996 - val_accuracy: 0.8725 - 20s/epoch - 113ms/step\n",
      "Epoch 2/10\n",
      "177/177 - 18s - loss: 0.3215 - accuracy: 0.9010 - val_loss: 0.5492 - val_accuracy: 0.7748 - 18s/epoch - 99ms/step\n",
      "Epoch 3/10\n",
      "177/177 - 18s - loss: 0.2614 - accuracy: 0.9156 - val_loss: 0.3136 - val_accuracy: 0.8915 - 18s/epoch - 100ms/step\n",
      "Epoch 4/10\n",
      "177/177 - 18s - loss: 0.2291 - accuracy: 0.9257 - val_loss: 0.2984 - val_accuracy: 0.8911 - 18s/epoch - 100ms/step\n",
      "Epoch 5/10\n",
      "177/177 - 18s - loss: 0.2075 - accuracy: 0.9294 - val_loss: 0.2818 - val_accuracy: 0.8948 - 18s/epoch - 99ms/step\n",
      "Epoch 6/10\n",
      "177/177 - 18s - loss: 0.1947 - accuracy: 0.9340 - val_loss: 0.3018 - val_accuracy: 0.8965 - 18s/epoch - 100ms/step\n",
      "Epoch 7/10\n",
      "177/177 - 18s - loss: 0.1834 - accuracy: 0.9374 - val_loss: 0.3087 - val_accuracy: 0.9014 - 18s/epoch - 100ms/step\n",
      "Epoch 8/10\n",
      "177/177 - 18s - loss: 0.1794 - accuracy: 0.9388 - val_loss: 0.2778 - val_accuracy: 0.8981 - 18s/epoch - 100ms/step\n",
      "Epoch 9/10\n",
      "177/177 - 18s - loss: 0.1664 - accuracy: 0.9452 - val_loss: 0.2881 - val_accuracy: 0.8952 - 18s/epoch - 100ms/step\n",
      "Epoch 10/10\n",
      "177/177 - 18s - loss: 0.1582 - accuracy: 0.9452 - val_loss: 0.2841 - val_accuracy: 0.8952 - 18s/epoch - 100ms/step\n",
      "Epoch 1/10\n",
      "177/177 - 20s - loss: 0.5321 - accuracy: 0.8350 - val_loss: 0.3802 - val_accuracy: 0.8812 - 20s/epoch - 113ms/step\n",
      "Epoch 2/10\n",
      "177/177 - 18s - loss: 0.3142 - accuracy: 0.8992 - val_loss: 0.4225 - val_accuracy: 0.8387 - 18s/epoch - 99ms/step\n",
      "Epoch 3/10\n",
      "177/177 - 18s - loss: 0.2542 - accuracy: 0.9151 - val_loss: 0.2882 - val_accuracy: 0.8969 - 18s/epoch - 99ms/step\n",
      "Epoch 4/10\n",
      "177/177 - 18s - loss: 0.2248 - accuracy: 0.9232 - val_loss: 0.2870 - val_accuracy: 0.8981 - 18s/epoch - 99ms/step\n",
      "Epoch 5/10\n",
      "177/177 - 18s - loss: 0.2086 - accuracy: 0.9298 - val_loss: 0.3015 - val_accuracy: 0.8977 - 18s/epoch - 99ms/step\n",
      "Epoch 6/10\n",
      "177/177 - 18s - loss: 0.1939 - accuracy: 0.9353 - val_loss: 0.2735 - val_accuracy: 0.8965 - 18s/epoch - 99ms/step\n",
      "Epoch 7/10\n",
      "177/177 - 18s - loss: 0.1816 - accuracy: 0.9381 - val_loss: 0.2886 - val_accuracy: 0.8985 - 18s/epoch - 99ms/step\n",
      "Epoch 8/10\n",
      "177/177 - 18s - loss: 0.1753 - accuracy: 0.9404 - val_loss: 0.3073 - val_accuracy: 0.9010 - 18s/epoch - 99ms/step\n",
      "Epoch 9/10\n",
      "177/177 - 18s - loss: 0.1651 - accuracy: 0.9432 - val_loss: 0.2694 - val_accuracy: 0.9010 - 18s/epoch - 99ms/step\n",
      "Epoch 10/10\n",
      "177/177 - 18s - loss: 0.1550 - accuracy: 0.9439 - val_loss: 0.2894 - val_accuracy: 0.8969 - 18s/epoch - 99ms/step\n",
      "Epoch 1/10\n",
      "177/177 - 20s - loss: 0.5139 - accuracy: 0.8414 - val_loss: 0.3790 - val_accuracy: 0.8771 - 20s/epoch - 113ms/step\n",
      "Epoch 2/10\n",
      "177/177 - 18s - loss: 0.3132 - accuracy: 0.8958 - val_loss: 0.3189 - val_accuracy: 0.8911 - 18s/epoch - 99ms/step\n",
      "Epoch 3/10\n",
      "177/177 - 18s - loss: 0.2535 - accuracy: 0.9176 - val_loss: 0.3187 - val_accuracy: 0.8907 - 18s/epoch - 99ms/step\n",
      "Epoch 4/10\n",
      "177/177 - 18s - loss: 0.2223 - accuracy: 0.9243 - val_loss: 0.2979 - val_accuracy: 0.8923 - 18s/epoch - 99ms/step\n",
      "Epoch 5/10\n",
      "177/177 - 18s - loss: 0.2072 - accuracy: 0.9296 - val_loss: 0.2724 - val_accuracy: 0.8977 - 18s/epoch - 99ms/step\n",
      "Epoch 6/10\n",
      "177/177 - 18s - loss: 0.1859 - accuracy: 0.9381 - val_loss: 0.2744 - val_accuracy: 0.8998 - 18s/epoch - 99ms/step\n",
      "Epoch 7/10\n",
      "177/177 - 18s - loss: 0.1814 - accuracy: 0.9339 - val_loss: 0.2971 - val_accuracy: 0.8998 - 18s/epoch - 99ms/step\n",
      "Epoch 8/10\n",
      "177/177 - 18s - loss: 0.1715 - accuracy: 0.9404 - val_loss: 0.2845 - val_accuracy: 0.8969 - 18s/epoch - 99ms/step\n",
      "Epoch 9/10\n",
      "177/177 - 18s - loss: 0.1702 - accuracy: 0.9411 - val_loss: 0.2830 - val_accuracy: 0.8956 - 18s/epoch - 100ms/step\n",
      "Epoch 10/10\n",
      "177/177 - 18s - loss: 0.1577 - accuracy: 0.9455 - val_loss: 0.2958 - val_accuracy: 0.8960 - 18s/epoch - 99ms/step\n"
     ]
    }
   ],
   "source": [
    "# Function to average the predictions of individual models\n",
    "def ensemble_predict(models, X):\n",
    "    predictions = [model.predict(X) for model in models]\n",
    "    avg_prediction = np.average(predictions, axis=0)\n",
    "    return avg_prediction\n",
    "\n",
    "# List to store the individual models\n",
    "models = []\n",
    "\n",
    "# Number of models to train\n",
    "num_models = 5\n",
    "\n",
    "# Creating base models for the bagging ensemble algorithm\n",
    "for i in range(num_models):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Embedding(max_features, 128, input_length=X_train.shape[1]),\n",
    "        tf.keras.layers.SpatialDropout1D(0.5),\n",
    "        tf.keras.layers.LSTM(196, dropout=0.3, recurrent_dropout=0.3),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(100, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dropout(0.4),\n",
    "        tf.keras.layers.Dense(2, activation=tf.nn.softmax,kernel_regularizer=regularizers.L1(0.01))\n",
    "    ])\n",
    "\n",
    "    # Compiling the model with the given best configuration procured earlier\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    # Fitting the model\n",
    "    model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.3, verbose=2)\n",
    "    # Adding the model to the list\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up parameter for the ensemble model\n",
    "batch_size = 32\n",
    "\n",
    "# Preparing input for the ensemble model\n",
    "ensemble_inputs = models[0].input\n",
    "input_shape = models[0].input_shape[1:]\n",
    "X = np.zeros((batch_size,) + input_shape)\n",
    "\n",
    "# Preparing output for the ensemble model\n",
    "ensemble_outputs = [model(ensemble_inputs) for model in models]\n",
    "average_outputs = tf.reduce_mean(ensemble_outputs, axis=0)\n",
    "average_outputs = tf.keras.layers.Lambda(lambda x: x)(average_outputs)\n",
    "\n",
    "# Creating the final ensemble model\n",
    "ensemble_model = tf.keras.models.Model(inputs=ensemble_inputs, outputs=average_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ensemble model with suitable configurations\n",
    "ensemble_model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "177/177 - 82s - loss: 0.1574 - accuracy: 0.9540 - val_loss: 0.3195 - val_accuracy: 0.8998 - 82s/epoch - 464ms/step\n",
      "Epoch 2/10\n",
      "177/177 - 82s - loss: 0.1496 - accuracy: 0.9568 - val_loss: 0.3392 - val_accuracy: 0.8940 - 82s/epoch - 465ms/step\n",
      "Epoch 3/10\n",
      "177/177 - 82s - loss: 0.1444 - accuracy: 0.9593 - val_loss: 0.3422 - val_accuracy: 0.8981 - 82s/epoch - 463ms/step\n",
      "Epoch 4/10\n",
      "177/177 - 82s - loss: 0.1416 - accuracy: 0.9586 - val_loss: 0.3276 - val_accuracy: 0.8936 - 82s/epoch - 464ms/step\n",
      "Epoch 5/10\n",
      "177/177 - 82s - loss: 0.1366 - accuracy: 0.9627 - val_loss: 0.3846 - val_accuracy: 0.8965 - 82s/epoch - 464ms/step\n",
      "Epoch 6/10\n",
      "177/177 - 82s - loss: 0.1306 - accuracy: 0.9641 - val_loss: 0.4071 - val_accuracy: 0.8919 - 82s/epoch - 463ms/step\n",
      "Epoch 7/10\n",
      "177/177 - 82s - loss: 0.1298 - accuracy: 0.9643 - val_loss: 0.3220 - val_accuracy: 0.8903 - 82s/epoch - 464ms/step\n",
      "Epoch 8/10\n",
      "177/177 - 82s - loss: 0.1215 - accuracy: 0.9669 - val_loss: 0.3762 - val_accuracy: 0.8915 - 82s/epoch - 464ms/step\n",
      "Epoch 9/10\n",
      "177/177 - 82s - loss: 0.1269 - accuracy: 0.9639 - val_loss: 0.3577 - val_accuracy: 0.8882 - 82s/epoch - 464ms/step\n",
      "Epoch 10/10\n",
      "177/177 - 82s - loss: 0.1141 - accuracy: 0.9678 - val_loss: 0.3972 - val_accuracy: 0.8907 - 82s/epoch - 463ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26d285c4730>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fitting the ensemble model\n",
    "history = ensemble_model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_split=0.3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 11s 98ms/step - loss: 0.3343 - accuracy: 0.9108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3343445956707001, 0.9107710123062134]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluating the ensemble model\n",
    "ensemble_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting loss vs accuracy (training and validation)\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss / Accuracy')\n",
    "plt.title('Training Loss and Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 11s 96ms/step\n",
      "Precision: 0.7769679300291545\n",
      "Recall: 0.7735849056603774\n",
      "F1-Score: 0.7752727272727273\n",
      "AUC: 0.9451228125984286\n"
     ]
    }
   ],
   "source": [
    "Y_pred = ensemble_model.predict(X_test)\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Getting other model metrics for the ensemble model\n",
    "precision = precision_score(Y_test, Y_pred_classes)\n",
    "recall = recall_score(Y_test, Y_pred_classes)\n",
    "f1 = f1_score(Y_test, Y_pred_classes)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Y_pred[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"AUC:\", roc_auc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we observed here, by using the bagging ensemble technique, the new model is able to reach an accuracy of **91.08%**, as compared to the **90.83%** accuracy provided by the best fit model by GridSearchCV"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost-Sensitive Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost-Sensitive Learning is a type of machine learning that takes into account the costs of different types of errors when making predictions. For example, if the cost of false negatives (prediction is negative, but the actual label is positive) is higher than the cost of false positives (prediction is positive, but the actual label is negative), the model will adjust its decision boundary to minimize the overall cost."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of a model is not always a good indicator of the performance of the model. It is important to consider the context and the problem that the model is trying to solve. In general, a higher accuracy is preferred, but sometimes a trade-off between accuracy and other performance measures, such as recall or precision, is necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights:  [1.26139913 4.8255675 ]\n",
      "Epoch 1/12\n",
      "76/76 - 60s - loss: 0.5260 - accuracy: 0.9629 - weighted_accuracy: 0.9629 - val_loss: 1.3635 - val_accuracy: 0.9112 - val_weighted_accuracy: 0.9112 - 60s/epoch - 785ms/step\n",
      "Epoch 2/12\n",
      "76/76 - 42s - loss: 0.4780 - accuracy: 0.9672 - weighted_accuracy: 0.9672 - val_loss: 1.3212 - val_accuracy: 0.9127 - val_weighted_accuracy: 0.9127 - 42s/epoch - 557ms/step\n",
      "Epoch 3/12\n",
      "76/76 - 42s - loss: 0.4880 - accuracy: 0.9693 - weighted_accuracy: 0.9693 - val_loss: 1.2754 - val_accuracy: 0.9097 - val_weighted_accuracy: 0.9097 - 42s/epoch - 557ms/step\n",
      "Epoch 4/12\n",
      "76/76 - 42s - loss: 0.4470 - accuracy: 0.9707 - weighted_accuracy: 0.9707 - val_loss: 1.5254 - val_accuracy: 0.9158 - val_weighted_accuracy: 0.9158 - 42s/epoch - 557ms/step\n",
      "Epoch 5/12\n",
      "76/76 - 42s - loss: 0.4349 - accuracy: 0.9728 - weighted_accuracy: 0.9728 - val_loss: 1.6959 - val_accuracy: 0.9016 - val_weighted_accuracy: 0.9016 - 42s/epoch - 558ms/step\n",
      "Epoch 6/12\n",
      "76/76 - 42s - loss: 0.4186 - accuracy: 0.9721 - weighted_accuracy: 0.9721 - val_loss: 1.4921 - val_accuracy: 0.9146 - val_weighted_accuracy: 0.9146 - 42s/epoch - 558ms/step\n",
      "Epoch 7/12\n",
      "76/76 - 42s - loss: 0.3973 - accuracy: 0.9746 - weighted_accuracy: 0.9746 - val_loss: 1.5750 - val_accuracy: 0.9100 - val_weighted_accuracy: 0.9100 - 42s/epoch - 559ms/step\n",
      "Epoch 8/12\n",
      "76/76 - 42s - loss: 0.3761 - accuracy: 0.9769 - weighted_accuracy: 0.9769 - val_loss: 1.7929 - val_accuracy: 0.9100 - val_weighted_accuracy: 0.9100 - 42s/epoch - 558ms/step\n",
      "Epoch 9/12\n",
      "76/76 - 42s - loss: 0.3853 - accuracy: 0.9761 - weighted_accuracy: 0.9761 - val_loss: 1.7975 - val_accuracy: 0.9093 - val_weighted_accuracy: 0.9093 - 42s/epoch - 559ms/step\n",
      "Epoch 10/12\n",
      "76/76 - 42s - loss: 0.3326 - accuracy: 0.9777 - weighted_accuracy: 0.9777 - val_loss: 1.6356 - val_accuracy: 0.9109 - val_weighted_accuracy: 0.9109 - 42s/epoch - 558ms/step\n",
      "Epoch 11/12\n",
      "76/76 - 43s - loss: 0.3408 - accuracy: 0.9765 - weighted_accuracy: 0.9765 - val_loss: 1.5861 - val_accuracy: 0.9059 - val_weighted_accuracy: 0.9059 - 43s/epoch - 570ms/step\n",
      "Epoch 12/12\n",
      "76/76 - 43s - loss: 0.3434 - accuracy: 0.9785 - weighted_accuracy: 0.9785 - val_loss: 1.7059 - val_accuracy: 0.9118 - val_weighted_accuracy: 0.9118 - 43s/epoch - 567ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1606e6172e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_weights(Y):\n",
    "    # Generating class weights for the binary classficiation model\n",
    "    class_weights = np.zeros(2)\n",
    "    for i in range(2):\n",
    "        class_weights[i] = 1.0 / (sum(Y == i)/len(Y))\n",
    "    return class_weights\n",
    "\n",
    "class_weights = get_weights(Y_train)\n",
    "print(\"Class Weights: \", class_weights)\n",
    "\n",
    "# Compiling the Cost-sensitive learning model with required parameters\n",
    "ensemble_model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"],\n",
    "sample_weight_mode=\"temporary\", weighted_metrics=[\"accuracy\"], loss_weights=class_weights[1])\n",
    "\n",
    "ensemble_model.fit(X_train, Y_train, epochs=12, batch_size=64, validation_split=0.4, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  1.780985951423645\n",
      "accuracy =  0.9113485217094421\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the Cost-sensitive learning model\n",
    "score = ensemble_model.evaluate(X_test, Y_test, verbose=False)\n",
    "\n",
    "print(\"loss = \", score[0])\n",
    "print(\"accuracy = \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 11s 94ms/step\n",
      "Precision: 0.7876506024096386\n",
      "Recall: 0.7590711175616836\n",
      "F1-Score: 0.7730968218773095\n",
      "AUC: 0.9410593181763482\n"
     ]
    }
   ],
   "source": [
    "Y_pred = ensemble_model.predict(X_test)\n",
    "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Getting other model metrics for the Cost-Sensitive learning model\n",
    "precision = precision_score(Y_test, Y_pred_classes)\n",
    "recall = recall_score(Y_test, Y_pred_classes)\n",
    "f1 = f1_score(Y_test, Y_pred_classes)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Y_pred[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"AUC:\", roc_auc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we observe that the Cost-sensitive learning model has given an accuracy of **89.77%** as compared to the **90.83%** accuracy of the original best fit model and the **91.08%** accuracy of the bagging ensemble model. In this case, normal accuracy (0.9083) is higher than the Cost-Sensitive Learning accuracy (0.8977), which suggests that the normal accuracy model is performing better. However, without additional information about the context, it is difficult to say for certain which model is better."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that the Cost-Sensitive Learning accuracy is lower, but the overall performance of the model is better, because it minimizes the costs associated with the errors. To determine which model is better, you would need to consider additional performance measures, such as precision, recall, F1-score, or the confusion matrix, and determine the overall trade-off between accuracy and costs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When it comes to choosing between these two techniques for a bagging ensemble of RNNs, it is important to consider the cost of misclassification and the degree of class imbalance in the dataset. If the cost of misclassification is high or the dataset is imbalanced, cost-sensitive learning may be a better option. However, if neither of these factors are a concern and the goal is to maximize overall accuracy, normal bagging ensemble may be sufficient.\n",
    "\n",
    "#### Ultimately, the best approach will depend on the specific characteristics of the problem and the goals of the project. It may be useful to experiment with both techniques and compare their performance on a validation set before making a final decision. Therefore, since an imbalance exists in our airline sentiment data and the significance given to the negative reviews should be higher, the Cost-Sensitive model is the best suitable model for our problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0674782e80fe76cdafa01c42e1ecf76036ae81de091792b76700c7fc0e3fe17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
